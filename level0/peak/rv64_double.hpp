/******************************************************************************
** Copyright (c) 2013-2018, Alexander Heinecke                               **
** All rights reserved.                                                      **
**                                                                           **
** Redistribution and use in source and binary forms, with or without        **
** modification, are permitted provided that the following conditions        **
** are met:                                                                  **
** 1. Redistributions of source code must retain the above copyright         **
**    notice, this list of conditions and the following disclaimer.          **
** 2. Redistributions in binary form must reproduce the above copyright      **
**    notice, this list of conditions and the following disclaimer in the    **
**    documentation and/or other materials provided with the distribution.   **
** 3. Neither the name of the copyright holder nor the names of its          **
**    contributors may be used to endorse or promote products derived        **
**    from this software without specific prior written permission.          **
**                                                                           **
** THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS       **
** "AS IS" AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT         **
** LIMITED TO, THE IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR     **
** A PARTICULAR PURPOSE ARE DISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT      **
** HOLDER OR CONTRIBUTORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL,    **
** SPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED  **
** TO, PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR    **
** PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY OF    **
** LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT (INCLUDING      **
** NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE OF THIS        **
** SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.              **
******************************************************************************/
#include <riscv_vector.h>

#define VLMAX (65536)

void gflops_double_qfma(double* data) {
    std::cout << "QFMA is not available on this architecture" << std::endl;
}

void gflops_double_fma(double* data) {
    int mvl;
    int rvl = VLMAX;

    __asm__ volatile ("vsetvli %0, %1, e64, m1, ta, ma\n\t"
                      "ld x18, %2\n\t"
                      "vle64.v v0, (x18)\n\t"
                      "vle64.v v1, (x18)\n\t"
                      "vle64.v v2, (x18)\n\t"
                      "vle64.v v3, (x18)\n\t"
                      "vle64.v v4, (x18)\n\t"
                      "vle64.v v5, (x18)\n\t"
                      "vle64.v v6, (x18)\n\t"
                      "vle64.v v7, (x18)\n\t"
                      "vle64.v v8, (x18)\n\t"
                      "vle64.v v9, (x18)\n\t"
                      "vle64.v v10, (x18)\n\t"
                      "vle64.v v11, (x18)\n\t"
                      "vle64.v v12, (x18)\n\t"
                      "vle64.v v13, (x18)\n\t"
                      "vle64.v v14, (x18)\n\t"
                      "vle64.v v15, (x18)\n\t"
                      "vle64.v v16, (x18)\n\t"
                      "vle64.v v17, (x18)\n\t"
                      "vle64.v v18, (x18)\n\t"
                      "vle64.v v19, (x18)\n\t"
                      "vle64.v v20, (x18)\n\t"
                      "vle64.v v21, (x18)\n\t"
                      "vle64.v v22, (x18)\n\t"
                      "vle64.v v23, (x18)\n\t"
                      "vle64.v v24, (x18)\n\t"
                      "vle64.v v25, (x18)\n\t"
                      "vle64.v v26, (x18)\n\t"
                      "vle64.v v27, (x18)\n\t"
                      "vle64.v v28, (x18)\n\t"
                      "vle64.v v29, (x18)\n\t"
                      "vle64.v v30, (x18)\n\t"
                      "vle64.v v31, (x18)\n\t"
                      "addi x19, x0, 100\n\t"
                      "mul x19, x19, x19\n\t"
                      "mul x19, x19, x19\n\t"
                      "dma_loop:"
                      "vfmacc.vv v0, v0, v0\n\t"
                      "vfmacc.vv v1, v1, v1\n\t"
                      "vfmacc.vv v2, v2, v2\n\t"
                      "vfmacc.vv v3, v3, v0\n\t"
                      "vfmacc.vv v4, v4, v0\n\t"
                      "vfmacc.vv v5, v5, v0\n\t"
                      "vfmacc.vv v6, v6, v0\n\t"
                      "vfmacc.vv v7, v7, v0\n\t"
                      "vfmacc.vv v8, v8, v0\n\t"
                      "vfmacc.vv v9, v9, v0\n\t"
                      "vfmacc.vv v10, v10, v10\n\t"
                      "vfmacc.vv v11, v11, v11\n\t"
                      "vfmacc.vv v12, v12, v12\n\t"
                      "vfmacc.vv v13, v13, v13\n\t"
                      "vfmacc.vv v14, v14, v14\n\t"
                      "vfmacc.vv v15, v15, v15\n\t"
                      "vfmacc.vv v16, v16, v16\n\t"
                      "vfmacc.vv v17, v17, v17\n\t"
                      "vfmacc.vv v18, v18, v18\n\t"
                      "vfmacc.vv v19, v19, v19\n\t"
                      "vfmacc.vv v20, v20, v20\n\t"
                      "vfmacc.vv v21, v21, v21\n\t"
                      "vfmacc.vv v22, v22, v22\n\t"
                      "vfmacc.vv v23, v23, v23\n\t"
                      "vfmacc.vv v24, v24, v24\n\t"
                      "vfmacc.vv v25, v25, v25\n\t"
                      "vfmacc.vv v26, v26, v26\n\t"
                      "vfmacc.vv v27, v27, v27\n\t"
                      "vfmacc.vv v28, v28, v28\n\t"
                      "vfmacc.vv v29, v29, v29\n\t"
                      "vfmacc.vv v30, v30, v30\n\t"
                      "vfmacc.vv v31, v31, v31\n\t"
                      "addi x19, x19, -1\n\t"
                      "bnez x19, dma_loop\n\t"
                      : "=r"(mvl) : "r" (rvl), "m"(data) : "x18", "x19", "v0", "v1", "v2", "v3", "v4", "v5", "v6", "v7", "v8", "v9", "v10", "v11", "v12", "v13", "v14", "v15", "v16", "v17", "v18", "v19", "v20", "v21", "v22", "v23", "v24", "v25", "v26", "v27", "v28", "v29", "v30", "v31" );
}

void gflops_double_add(double* data) {
    int mvl;
    int rvl = VLMAX;

    __asm__ volatile ("vsetvli %0, %1, e64, m1, ta, ma\n\t"
                      "ld x18, %2\n\t"
                      "vle64.v v0, (x18)\n\t"
                      "vle64.v v1, (x18)\n\t"
                      "vle64.v v2, (x18)\n\t"
                      "vle64.v v3, (x18)\n\t"
                      "vle64.v v4, (x18)\n\t"
                      "vle64.v v5, (x18)\n\t"
                      "vle64.v v6, (x18)\n\t"
                      "vle64.v v7, (x18)\n\t"
                      "vle64.v v8, (x18)\n\t"
                      "vle64.v v9, (x18)\n\t"
                      "vle64.v v10, (x18)\n\t"
                      "vle64.v v11, (x18)\n\t"
                      "vle64.v v12, (x18)\n\t"
                      "vle64.v v13, (x18)\n\t"
                      "vle64.v v14, (x18)\n\t"
                      "vle64.v v15, (x18)\n\t"
                      "vle64.v v16, (x18)\n\t"
                      "vle64.v v17, (x18)\n\t"
                      "vle64.v v18, (x18)\n\t"
                      "vle64.v v19, (x18)\n\t"
                      "vle64.v v20, (x18)\n\t"
                      "vle64.v v21, (x18)\n\t"
                      "vle64.v v22, (x18)\n\t"
                      "vle64.v v23, (x18)\n\t"
                      "vle64.v v24, (x18)\n\t"
                      "vle64.v v25, (x18)\n\t"
                      "vle64.v v26, (x18)\n\t"
                      "vle64.v v27, (x18)\n\t"
                      "vle64.v v28, (x18)\n\t"
                      "vle64.v v29, (x18)\n\t"
                      "vle64.v v30, (x18)\n\t"
                      "vle64.v v31, (x18)\n\t"
                      "addi x19, x0, 100\n\t"
                      "mul x19, x19, x19\n\t"
                      "mul x19, x19, x19\n\t"
                      "dadd_loop:"
                      "vfadd.vv v0, v0, v0\n\t"
                      "vfadd.vv v1, v1, v1\n\t"
                      "vfadd.vv v2, v2, v2\n\t"
                      "vfadd.vv v3, v3, v0\n\t"
                      "vfadd.vv v4, v4, v0\n\t"
                      "vfadd.vv v5, v5, v0\n\t"
                      "vfadd.vv v6, v6, v0\n\t"
                      "vfadd.vv v7, v7, v0\n\t"
                      "vfadd.vv v8, v8, v0\n\t"
                      "vfadd.vv v9, v9, v0\n\t"
                      "vfadd.vv v10, v10, v10\n\t"
                      "vfadd.vv v11, v11, v11\n\t"
                      "vfadd.vv v12, v12, v12\n\t"
                      "vfadd.vv v13, v13, v13\n\t"
                      "vfadd.vv v14, v14, v14\n\t"
                      "vfadd.vv v15, v15, v15\n\t"
                      "vfadd.vv v16, v16, v16\n\t"
                      "vfadd.vv v17, v17, v17\n\t"
                      "vfadd.vv v18, v18, v18\n\t"
                      "vfadd.vv v19, v19, v19\n\t"
                      "vfadd.vv v20, v20, v20\n\t"
                      "vfadd.vv v21, v21, v21\n\t"
                      "vfadd.vv v22, v22, v22\n\t"
                      "vfadd.vv v23, v23, v23\n\t"
                      "vfadd.vv v24, v24, v24\n\t"
                      "vfadd.vv v25, v25, v25\n\t"
                      "vfadd.vv v26, v26, v26\n\t"
                      "vfadd.vv v27, v27, v27\n\t"
                      "vfadd.vv v28, v28, v28\n\t"
                      "vfadd.vv v29, v29, v29\n\t"
                      "vfadd.vv v30, v30, v30\n\t"
                      "vfadd.vv v31, v31, v31\n\t"
                      "addi x19, x19, -1\n\t"
                      "bnez x19, dadd_loop\n\t"
                      : "=r"(mvl) : "r" (rvl), "m"(data) : "x18", "x19", "v0", "v1", "v2", "v3", "v4", "v5", "v6", "v7", "v8", "v9", "v10", "v11", "v12", "v13", "v14", "v15", "v16", "v17", "v18", "v19", "v20", "v21", "v22", "v23", "v24", "v25", "v26", "v27", "v28", "v29", "v30", "v31" );
}

void gflops_double_mul(double* data) {
    int mvl;
    int rvl = VLMAX;

    __asm__ volatile ("vsetvli %0, %1, e64, m1, ta, ma\n\t"
                      "ld x18, %2\n\t"
                      "vle64.v v0, (x18)\n\t"
                      "vle64.v v1, (x18)\n\t"
                      "vle64.v v2, (x18)\n\t"
                      "vle64.v v3, (x18)\n\t"
                      "vle64.v v4, (x18)\n\t"
                      "vle64.v v5, (x18)\n\t"
                      "vle64.v v6, (x18)\n\t"
                      "vle64.v v7, (x18)\n\t"
                      "vle64.v v8, (x18)\n\t"
                      "vle64.v v9, (x18)\n\t"
                      "vle64.v v10, (x18)\n\t"
                      "vle64.v v11, (x18)\n\t"
                      "vle64.v v12, (x18)\n\t"
                      "vle64.v v13, (x18)\n\t"
                      "vle64.v v14, (x18)\n\t"
                      "vle64.v v15, (x18)\n\t"
                      "vle64.v v16, (x18)\n\t"
                      "vle64.v v17, (x18)\n\t"
                      "vle64.v v18, (x18)\n\t"
                      "vle64.v v19, (x18)\n\t"
                      "vle64.v v20, (x18)\n\t"
                      "vle64.v v21, (x18)\n\t"
                      "vle64.v v22, (x18)\n\t"
                      "vle64.v v23, (x18)\n\t"
                      "vle64.v v24, (x18)\n\t"
                      "vle64.v v25, (x18)\n\t"
                      "vle64.v v26, (x18)\n\t"
                      "vle64.v v27, (x18)\n\t"
                      "vle64.v v28, (x18)\n\t"
                      "vle64.v v29, (x18)\n\t"
                      "vle64.v v30, (x18)\n\t"
                      "vle64.v v31, (x18)\n\t"
                      "addi x19, x0, 100\n\t"
                      "mul x19, x19, x19\n\t"
                      "mul x19, x19, x19\n\t"
                      "dmul_loop:"
                      "vfmul.vv v0, v0, v0\n\t"
                      "vfmul.vv v1, v1, v1\n\t"
                      "vfmul.vv v2, v2, v2\n\t"
                      "vfmul.vv v3, v3, v0\n\t"
                      "vfmul.vv v4, v4, v0\n\t"
                      "vfmul.vv v5, v5, v0\n\t"
                      "vfmul.vv v6, v6, v0\n\t"
                      "vfmul.vv v7, v7, v0\n\t"
                      "vfmul.vv v8, v8, v0\n\t"
                      "vfmul.vv v9, v9, v0\n\t"
                      "vfmul.vv v10, v10, v10\n\t"
                      "vfmul.vv v11, v11, v11\n\t"
                      "vfmul.vv v12, v12, v12\n\t"
                      "vfmul.vv v13, v13, v13\n\t"
                      "vfmul.vv v14, v14, v14\n\t"
                      "vfmul.vv v15, v15, v15\n\t"
                      "vfmul.vv v16, v16, v16\n\t"
                      "vfmul.vv v17, v17, v17\n\t"
                      "vfmul.vv v18, v18, v18\n\t"
                      "vfmul.vv v19, v19, v19\n\t"
                      "vfmul.vv v20, v20, v20\n\t"
                      "vfmul.vv v21, v21, v21\n\t"
                      "vfmul.vv v22, v22, v22\n\t"
                      "vfmul.vv v23, v23, v23\n\t"
                      "vfmul.vv v24, v24, v24\n\t"
                      "vfmul.vv v25, v25, v25\n\t"
                      "vfmul.vv v26, v26, v26\n\t"
                      "vfmul.vv v27, v27, v27\n\t"
                      "vfmul.vv v28, v28, v28\n\t"
                      "vfmul.vv v29, v29, v29\n\t"
                      "vfmul.vv v30, v30, v30\n\t"
                      "vfmul.vv v31, v31, v31\n\t"
                      "addi x19, x19, -1\n\t"
                      "bnez x19, dmul_loop\n\t"
                      : "=r"(mvl) : "r" (rvl), "m"(data) : "x18", "x19", "v0", "v1", "v2", "v3", "v4", "v5", "v6", "v7", "v8", "v9", "v10", "v11", "v12", "v13", "v14", "v15", "v16", "v17", "v18", "v19", "v20", "v21", "v22", "v23", "v24", "v25", "v26", "v27", "v28", "v29", "v30", "v31" );
}

void gflops_double_madd(double* data) {
    int mvl;
    int rvl = VLMAX;

    __asm__ volatile ("vsetvli %0, %1, e64, m1, ta, ma\n\t"
                      "ld x18, %2\n\t"
                      "vle64.v v0, (x18)\n\t"
                      "vle64.v v1, (x18)\n\t"
                      "vle64.v v2, (x18)\n\t"
                      "vle64.v v3, (x18)\n\t"
                      "vle64.v v4, (x18)\n\t"
                      "vle64.v v5, (x18)\n\t"
                      "vle64.v v6, (x18)\n\t"
                      "vle64.v v7, (x18)\n\t"
                      "vle64.v v8, (x18)\n\t"
                      "vle64.v v9, (x18)\n\t"
                      "vle64.v v10, (x18)\n\t"
                      "vle64.v v11, (x18)\n\t"
                      "vle64.v v12, (x18)\n\t"
                      "vle64.v v13, (x18)\n\t"
                      "vle64.v v14, (x18)\n\t"
                      "vle64.v v15, (x18)\n\t"
                      "vle64.v v16, (x18)\n\t"
                      "vle64.v v17, (x18)\n\t"
                      "vle64.v v18, (x18)\n\t"
                      "vle64.v v19, (x18)\n\t"
                      "vle64.v v20, (x18)\n\t"
                      "vle64.v v21, (x18)\n\t"
                      "vle64.v v22, (x18)\n\t"
                      "vle64.v v23, (x18)\n\t"
                      "vle64.v v24, (x18)\n\t"
                      "vle64.v v25, (x18)\n\t"
                      "vle64.v v26, (x18)\n\t"
                      "vle64.v v27, (x18)\n\t"
                      "vle64.v v28, (x18)\n\t"
                      "vle64.v v29, (x18)\n\t"
                      "vle64.v v30, (x18)\n\t"
                      "vle64.v v31, (x18)\n\t"
                      "addi x19, x0, 100\n\t"
                      "mul x19, x19, x19\n\t"
                      "mul x19, x19, x19\n\t"
                      "dmadd_loop:"
                      "vfmul.vv v0, v0, v0\n\t"
                      "vfadd.vv v1, v1, v1\n\t"
                      "vfmul.vv v2, v2, v2\n\t"
                      "vfadd.vv v3, v3, v0\n\t"
                      "vfmul.vv v4, v4, v0\n\t"
                      "vfadd.vv v5, v5, v0\n\t"
                      "vfmul.vv v6, v6, v0\n\t"
                      "vfadd.vv v7, v7, v0\n\t"
                      "vfmul.vv v8, v8, v0\n\t"
                      "vfadd.vv v9, v9, v0\n\t"
                      "vfmul.vv v10, v10, v10\n\t"
                      "vfadd.vv v11, v11, v11\n\t"
                      "vfmul.vv v12, v12, v12\n\t"
                      "vfadd.vv v13, v13, v13\n\t"
                      "vfmul.vv v14, v14, v14\n\t"
                      "vfadd.vv v15, v15, v15\n\t"
                      "vfmul.vv v16, v16, v16\n\t"
                      "vfadd.vv v17, v17, v17\n\t"
                      "vfmul.vv v18, v18, v18\n\t"
                      "vfadd.vv v19, v19, v19\n\t"
                      "vfmul.vv v20, v20, v20\n\t"
                      "vfadd.vv v21, v21, v21\n\t"
                      "vfmul.vv v22, v22, v22\n\t"
                      "vfadd.vv v23, v23, v23\n\t"
                      "vfmul.vv v24, v24, v24\n\t"
                      "vfadd.vv v25, v25, v25\n\t"
                      "vfmul.vv v26, v26, v26\n\t"
                      "vfadd.vv v27, v27, v27\n\t"
                      "vfmul.vv v28, v28, v28\n\t"
                      "vfadd.vv v29, v29, v29\n\t"
                      "vfmul.vv v30, v30, v30\n\t"
                      "vfadd.vv v31, v31, v31\n\t"
                      "addi x19, x19, -1\n\t"
                      "bnez x19, dmadd_loop\n\t"
                      : "=r"(mvl) : "r" (rvl), "m"(data) : "x18", "x19", "v0", "v1", "v2", "v3", "v4", "v5", "v6", "v7", "v8", "v9", "v10", "v11", "v12", "v13", "v14", "v15", "v16", "v17", "v18", "v19", "v20", "v21", "v22", "v23", "v24", "v25", "v26", "v27", "v28", "v29", "v30", "v31" );
}
